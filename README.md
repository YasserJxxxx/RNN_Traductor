# ğŸ‡ªğŸ‡¸â¡ï¸ğŸ‡¬ğŸ‡§ Traductor RNN (Seq2Seq) EspaÃ±ol â†’ InglÃ©s

Este proyecto presenta un **traductor automÃ¡tico de oraciones del espaÃ±ol al inglÃ©s**, desarrollado utilizando una **Red Neuronal Recurrente (RNN)** bajo una arquitectura **Seq2Seq (Sequence-to-Sequence)** con celdas **LSTM (Long Short-Term Memory)**.  
El sistema ha sido diseÃ±ado y optimizado para ejecutarse en **Google Colab**, aprovechando la integraciÃ³n con **Google Drive** para el almacenamiento y recuperaciÃ³n automÃ¡tica de los pesos del modelo.

El propÃ³sito de este proyecto es proporcionar un ejemplo educativo y prÃ¡ctico de cÃ³mo funcionan los traductores neuronales a nivel bÃ¡sico, sin depender de modelos preentrenados masivos como los de Google Translate o DeepL, permitiendo asÃ­ comprender cada etapa del pipeline de traducciÃ³n.

---

## ğŸš€ CaracterÃ­sticas Principales

âœ… Implementa un **modelo Seq2Seq completo** (encoder + decoder) con LSTM.  
âœ… Soluciona el **colapso del modelo** mediante normalizaciÃ³n de texto y preprocesamiento robusto.  
âœ… **Carga y guardado automÃ¡tico de pesos** en Google Drive, evitando repetir entrenamientos.  
âœ… Modo de **traducciÃ³n interactiva** desde la terminal o consola de Colab.  
âœ… Entrenamiento configurable con parÃ¡metros como nÃºmero de muestras, Ã©pocas y dimensiÃ³n latente.  
âœ… CÃ³digo totalmente documentado, modular y adaptable a otros pares de idiomas.  
âœ… Compatible con GPU de Colab para acelerar el entrenamiento.  

---

## ğŸ§  Arquitectura del Modelo

El traductor utiliza una arquitectura **Encoderâ€“Decoder**, que es un paradigma estÃ¡ndar en tareas de traducciÃ³n automÃ¡tica neuronal (NMT - Neural Machine Translation).

### ğŸ”¹ 1. Encoder (Codificador)
Procesa la secuencia de entrada en espaÃ±ol carÃ¡cter por carÃ¡cter (vectorizada en formato one-hot).  
Resume toda la informaciÃ³n semÃ¡ntica de la frase en dos vectores de estado:  
  - state_h: estado oculto del LSTM  
  - state_c: estado de la celda del LSTM  
Estos estados son la representaciÃ³n abstracta de la oraciÃ³n en el espacio latente.

### ğŸ”¹ 2. Decoder (Decodificador)
Recibe los estados finales del encoder como entrada inicial.  
Genera la secuencia de salida (en inglÃ©s) carÃ¡cter por carÃ¡cter, prediciendo cada sÃ­mbolo de forma autoregresiva.  
Durante el entrenamiento utiliza la tÃ©cnica de **Teacher Forcing**, donde cada paso recibe como entrada el carÃ¡cter correcto previo en lugar de su propia predicciÃ³n.

### ğŸ”¹ 3. Inferencia (TraducciÃ³n en tiempo real)
Se crean modelos separados para **inferencia**, permitiendo traducir frases nuevas sin volver a entrenar.  
Se genera la traducciÃ³n carÃ¡cter a carÃ¡cter hasta que se predice el token de fin (\n).  

---

## ğŸ“‚ Estructura del Proyecto
plaintext


ğŸ“ Traductor_RNN_EspaÃ±ol_InglÃ©s/
â”‚
â”œâ”€â”€ traductor_rnn_es_en.py           # CÃ³digo principal del proyecto
â”œâ”€â”€ README.md                        # DocumentaciÃ³n del repositorio
â”œâ”€â”€ spa-eng.zip                      # Dataset descargado automÃ¡ticamente
â”œâ”€â”€ /spa-eng/spa.txt                 # Dataset descomprimido (pares espaÃ±ol-inglÃ©s)
â””â”€â”€ /drive/MyDrive/Colab_Modelos/    # Carpeta de Google Drive para pesos del modelo
