{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasserJxxxx/RNN_Traductor/blob/main/Traductor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhpHTqUgn0pU",
        "outputId": "740daaa4-2e0a-412f-b2ab-c9063a9fcbc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Montando Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive montado con Ã©xito.\n",
            "\n",
            "âœ… Dataset encontrado y precargado en Drive: /content/drive/MyDrive/KaggleDatasets/englishspanish-translation-dataset/data.csv\n",
            "\n",
            "--- Resumen de Datos ---\n",
            "Total de muestras: 10000\n",
            "TamaÃ±o del vocabulario de entrada (InglÃ©s): 71\n",
            "TamaÃ±o del vocabulario de salida (EspaÃ±ol): 86\n",
            "Longitud mÃ¡xima de secuencia de entrada: 17\n",
            "Longitud mÃ¡xima de secuencia de salida: 42\n",
            "\n",
            "--- Entrenamiento Inicial (5 Ã‰pocas RÃ¡pidas) ---\n",
            "Epoch 1/5\n",
            "\u001b[1m141/141\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 324ms/step - loss: 1.4491\n",
            "Epoch 2/5\n",
            "\u001b[1m141/141\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 322ms/step - loss: 1.2746\n",
            "Epoch 3/5\n",
            "\u001b[1m141/141\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 311ms/step - loss: 1.2661\n",
            "Epoch 4/5\n",
            "\u001b[1m141/141\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 1.2559\n",
            "Epoch 5/5\n",
            "\u001b[1m141/141\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 312ms/step - loss: 1.2368\n",
            "\n",
            "Modelo inicial entrenado y listo para la traducciÃ³n interactiva.\n",
            "\n",
            "--- INICIO DEL BUCLE INTERACTIVO (APRENDIZAJE CONTINUO) ---\n",
            "Introduce 'salir' o 'quit' para terminar.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# --- 1. CONFIGURACIÃ“N ---\n",
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 256\n",
        "MAX_SAMPLES = 10000\n",
        "EPOCHS_PER_INTERACTION = 1\n",
        "KAGGLE_DATASET = \"lonnieqin/englishspanish-translation-dataset\"\n",
        "POSSIBLE_EXTENSIONS = ['.csv', '.txt']\n",
        "\n",
        "# Variables globales que serÃ¡n definidas despuÃ©s de la funciÃ³n preprocess_data\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "num_encoder_tokens = 0\n",
        "num_decoder_tokens = 0\n",
        "max_encoder_seq_length = 0\n",
        "max_decoder_seq_length = 0\n",
        "input_token_index = {}\n",
        "target_token_index = {}\n",
        "reverse_target_char_index = {}\n",
        "\n",
        "\n",
        "# --- 2. GESTIÃ“N Y CARGA DEL DATASET DESDE KAGGLE/DRIVE ---\n",
        "\n",
        "def load_data_from_kaggle_or_drive(dataset_name):\n",
        "    \"\"\"Monta Drive, descarga el dataset si no existe, y devuelve la ruta del archivo de datos.\"\"\"\n",
        "\n",
        "    print(\"Montando Google Drive...\")\n",
        "    # Permite montar el Drive aunque ya estÃ© montado\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive montado con Ã©xito.\")\n",
        "\n",
        "    # Definir rutas\n",
        "    DRIVE_BASE_PATH = '/content/drive/MyDrive/KaggleDatasets'\n",
        "    DATASET_FOLDER = dataset_name.split('/')[-1]\n",
        "    FINAL_PATH = os.path.join(DRIVE_BASE_PATH, DATASET_FOLDER)\n",
        "\n",
        "    # Crear carpeta si no existe\n",
        "    os.makedirs(FINAL_PATH, exist_ok=True)\n",
        "\n",
        "    # Buscar el archivo de datos precargado en Drive\n",
        "    for root, dirs, files in os.walk(FINAL_PATH):\n",
        "        for name in files:\n",
        "            if any(name.endswith(ext) for ext in POSSIBLE_EXTENSIONS):\n",
        "                full_path_in_drive = os.path.join(root, name)\n",
        "                print(f\"\\nâœ… Dataset encontrado y precargado en Drive: {full_path_in_drive}\")\n",
        "                return full_path_in_drive\n",
        "\n",
        "    # 3. Descargar si el archivo NO existe en Drive\n",
        "    print(f\"\\nArchivo no encontrado en Drive. Descargando '{dataset_name}' a la cachÃ© de Colab...\")\n",
        "    try:\n",
        "        local_cache_path = kagglehub.dataset_download(dataset_name)\n",
        "\n",
        "        found_file = None\n",
        "        for root, dirs, files in os.walk(local_cache_path):\n",
        "            for name in files:\n",
        "                if any(name.endswith(ext) for ext in POSSIBLE_EXTENSIONS):\n",
        "                    found_file = os.path.join(root, name)\n",
        "                    break\n",
        "            if found_file:\n",
        "                break\n",
        "\n",
        "        if found_file:\n",
        "            file_name = os.path.basename(found_file)\n",
        "            full_path_in_drive = os.path.join(FINAL_PATH, file_name)\n",
        "\n",
        "            shutil.move(found_file, full_path_in_drive)\n",
        "            print(f\"âœ… Descarga completada y movida a Google Drive para persistencia.\")\n",
        "            return full_path_in_drive\n",
        "        else:\n",
        "             print(f\"âŒ ERROR: No se encontrÃ³ ningÃºn archivo .csv o .txt en el directorio descargado.\")\n",
        "             return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR: FallÃ³ la descarga o el movimiento del dataset. Detalle: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- EJECUTAR CARGA INICIAL ---\n",
        "data_path = load_data_from_kaggle_or_drive(KAGGLE_DATASET)\n",
        "\n",
        "if not data_path:\n",
        "    print(\"\\nPrograma terminado debido a la falta del dataset.\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- 3. PREPROCESAMIENTO DE DATOS ---\n",
        "\n",
        "def preprocess_data(path, max_samples):\n",
        "    \"\"\"Carga y tokeniza el dataset, devolviendo todos los mapeos y longitudes.\"\"\"\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    input_characters = set()\n",
        "    target_characters = set()\n",
        "\n",
        "    # Cargar datos usando pandas (lÃ³gica de detecciÃ³n de separador)\n",
        "    try:\n",
        "        df = None\n",
        "        for sep in ['|', '\\t', ',', ';']:\n",
        "            try:\n",
        "                # Intentar el formato mÃ¡s comÃºn (3 columnas, usamos 1 y 2)\n",
        "                df = pd.read_csv(path, sep=sep, encoding='utf-8', header=None, on_bad_lines='skip', usecols=[1, 2])\n",
        "                break\n",
        "            except Exception:\n",
        "                # Intentar el formato bÃ¡sico (2 columnas, usamos 0 y 1)\n",
        "                try:\n",
        "                    df = pd.read_csv(path, sep=sep, encoding='utf-8', header=None, on_bad_lines='skip', usecols=[0, 1])\n",
        "                    break\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            raise ValueError(\"No se pudo leer el archivo con delimitadores estÃ¡ndar.\")\n",
        "\n",
        "        input_texts_raw = df.iloc[:, 0].astype(str).tolist()\n",
        "        target_texts_raw = df.iloc[:, 1].astype(str).tolist()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: FallÃ³ la lectura del archivo '{path}'. Detalle: {e}\")\n",
        "        sys.exit()\n",
        "\n",
        "    # Procesamiento del texto\n",
        "    for input_text, target_text in zip(input_texts_raw[:min(max_samples, len(input_texts_raw))],\n",
        "                                      target_texts_raw[:min(max_samples, len(target_texts_raw))]):\n",
        "\n",
        "        input_text = input_text.strip()\n",
        "        target_text = '\\t' + target_text.strip() + '\\n' # Token de inicio y fin\n",
        "\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "        for char in input_text:\n",
        "            if char not in input_characters:\n",
        "                input_characters.add(char)\n",
        "        for char in target_text:\n",
        "            if char not in target_characters:\n",
        "                target_characters.add(char)\n",
        "\n",
        "    # --- Generar Mapeos y Longitudes ---\n",
        "\n",
        "    input_characters = sorted(list(input_characters))\n",
        "    target_characters = sorted(list(target_characters))\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts if txt]) if input_texts else 1\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts if txt]) if target_texts else 1\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "    # Devolvemos todos los resultados\n",
        "    return (input_texts, target_texts,\n",
        "            input_token_index, target_token_index, reverse_target_char_index,\n",
        "            num_encoder_tokens, num_decoder_tokens,\n",
        "            max_encoder_seq_length, max_decoder_seq_length)\n",
        "\n",
        "# --- LLAMADA A PREPROCESS_DATA Y ASIGNACIÃ“N DE GLOBALES ---\n",
        "\n",
        "(input_texts, target_texts,\n",
        " input_token_index, target_token_index, reverse_target_char_index,\n",
        " num_encoder_tokens, num_decoder_tokens,\n",
        " max_encoder_seq_length, max_decoder_seq_length) = preprocess_data(data_path, MAX_SAMPLES)\n",
        "\n",
        "print(f\"\\n--- Resumen de Datos ---\")\n",
        "print(f\"Total de muestras: {len(input_texts)}\")\n",
        "print(f\"TamaÃ±o del vocabulario de entrada (InglÃ©s): {num_encoder_tokens}\")\n",
        "print(f\"TamaÃ±o del vocabulario de salida (EspaÃ±ol): {num_decoder_tokens}\")\n",
        "print(f\"Longitud mÃ¡xima de secuencia de entrada: {max_encoder_seq_length}\")\n",
        "print(f\"Longitud mÃ¡xima de secuencia de salida: {max_decoder_seq_length}\")\n",
        "\n",
        "\n",
        "def vectorize_data(input_texts_list, target_texts_list):\n",
        "    \"\"\"Convierte las secuencias de texto a matrices one-hot.\"\"\"\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts_list), max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype='float32')\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts_list), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype='float32')\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts_list), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype='float32')\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts_list, target_texts_list)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            if char in input_token_index:\n",
        "                encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "        for t, char in enumerate(target_text):\n",
        "            if char in target_token_index:\n",
        "                decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "                if t > 0:\n",
        "                    decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
        "\n",
        "# Dividir y vectorizar para entrenamiento inicial\n",
        "input_train, _, target_train, _ = train_test_split(\n",
        "    input_texts, target_texts, test_size=0.1, random_state=42)\n",
        "\n",
        "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(\n",
        "    input_train, target_train)\n",
        "\n",
        "\n",
        "# --- 4. MODELO SEQ2SEQ Y FUNCIONES DE INFERENCIA ---\n",
        "\n",
        "def create_seq2seq_model():\n",
        "    \"\"\"Define y compila el modelo Seq2Seq y sus modelos de inferencia.\"\"\"\n",
        "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = LSTM(LATENT_DIM, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "    decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
        "    decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "# Crear y entrenar inicialmente el modelo\n",
        "model, encoder_model, decoder_model = create_seq2seq_model()\n",
        "\n",
        "print(\"\\n--- Entrenamiento Inicial (5 Ã‰pocas RÃ¡pidas) ---\")\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=5,\n",
        "          verbose=1)\n",
        "\n",
        "print(\"\\nModelo inicial entrenado y listo para la traducciÃ³n interactiva.\")\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    \"\"\"Decodifica la secuencia de entrada (InglÃ©s) a la de salida (EspaÃ±ol).\"\"\"\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        if sampled_token_index not in reverse_target_char_index:\n",
        "             sampled_char = ''\n",
        "        else:\n",
        "             sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            break\n",
        "\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "\n",
        "def get_input_seq(text):\n",
        "    \"\"\"Convierte el texto del usuario a la secuencia one-hot necesaria.\"\"\"\n",
        "    text = text[:max_encoder_seq_length]\n",
        "\n",
        "    encoder_input_data_user = np.zeros(\n",
        "        (1, max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype='float32')\n",
        "\n",
        "    for t, char in enumerate(text):\n",
        "        if char in input_token_index:\n",
        "            encoder_input_data_user[0, t, input_token_index[char]] = 1.\n",
        "\n",
        "    return encoder_input_data_user\n",
        "\n",
        "# --- 5. BUCLE INFINITO DE TRADUCCIÃ“N Y APRENDIZAJE ---\n",
        "\n",
        "print(\"\\n--- INICIO DEL BUCLE INTERACTIVO (APRENDIZAJE CONTINUO) ---\")\n",
        "print(\"Introduce 'salir' o 'quit' para terminar.\")\n",
        "\n",
        "new_input_texts = []\n",
        "new_target_texts = []\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        input_sentence = input(\"\\nIntroduce frase u oraciÃ³n (InglÃ©s): \").strip()\n",
        "\n",
        "        if input_sentence.lower() in ['salir', 'quit']:\n",
        "            print(\"Cerrando el bucle interactivo. Â¡AdiÃ³s!\")\n",
        "            break\n",
        "\n",
        "        if not input_sentence:\n",
        "            continue\n",
        "\n",
        "        input_seq = get_input_seq(input_sentence)\n",
        "        translated_text = decode_sequence(input_seq)\n",
        "\n",
        "        print(f\"TraducciÃ³n del modelo: **{translated_text}**\")\n",
        "\n",
        "        is_correct = input(\"Â¿Es correcta la traducciÃ³n (s/n)? \").strip().lower()\n",
        "\n",
        "        if is_correct == 'n':\n",
        "            correct_translation = input(\"Introduce la traducciÃ³n correcta (EspaÃ±ol): \").strip()\n",
        "\n",
        "            if correct_translation:\n",
        "                target_text_corrected = '\\t' + correct_translation + '\\n'\n",
        "\n",
        "                new_input_texts.append(input_sentence)\n",
        "                new_target_texts.append(target_text_corrected)\n",
        "\n",
        "                print(\"ðŸ‘ Par de correcciÃ³n almacenado.\")\n",
        "\n",
        "                if len(new_input_texts) >= BATCH_SIZE:\n",
        "                    print(f\"\\n--- Re-entrenando con {len(new_input_texts)} nuevos pares (EPOCHS={EPOCHS_PER_INTERACTION}) ---\")\n",
        "\n",
        "                    new_encoder_data, new_decoder_input, new_decoder_target = vectorize_data(\n",
        "                        new_input_texts, new_target_texts)\n",
        "\n",
        "                    model.fit([new_encoder_data, new_decoder_input], new_decoder_target,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              epochs=EPOCHS_PER_INTERACTION,\n",
        "                              verbose=0)\n",
        "\n",
        "                    print(\"âœ… Modelo actualizado. Â¡AprendÃ­ de mis errores!\")\n",
        "                    new_input_texts = []\n",
        "                    new_target_texts = []\n",
        "\n",
        "        elif is_correct == 's':\n",
        "            print(\"Â¡Excelente! TraducciÃ³n validada.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Respuesta no vÃ¡lida. Saltando al siguiente intento.\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCerrando el bucle interactivo por interrupciÃ³n del teclado. Â¡AdiÃ³s!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\nOcurriÃ³ un error inesperado: {e}\")\n",
        "        print(\"Continuando con el ciclo.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yD4oeBXag18g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}