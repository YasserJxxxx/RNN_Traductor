{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxQHYh+wWrXnyNCtybNLTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasserJxxxx/RNN_Traductor/blob/main/traductor3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# --- 1. Configuraci√≥n ---\n",
        "# El nombre exacto de tu archivo subido\n",
        "FILE_IN = \"Parejas de oraciones en Espa√±olIngl√©s - 2025-11-18.tsv\"\n",
        "\n",
        "# ¬°CAMBIO! El archivo de salida ahora es de 1 mill√≥n\n",
        "FILE_OUT = \"tatoeba_limpio_1M.csv\"\n",
        "# ¬°CAMBIO! Aumentamos el n√∫mero de muestras\n",
        "N_SAMPLES = 1000000\n",
        "\n",
        "# --- 2. Funci√≥n de limpieza (sin tildes) ---\n",
        "def limpiar_texto_sin_tildes(texto):\n",
        "    texto = str(texto).lower()\n",
        "    # Quitar tildes\n",
        "    texto_normalizado = unicodedata.normalize('NFD', texto)\n",
        "    texto_sin_tildes = texto_normalizado.encode('ascii', 'ignore').decode('utf-8')\n",
        "    # Limpiar puntuaci√≥n\n",
        "    texto_sin_tildes = re.sub(r\"([?.!,¬ø])\", r\" \\1 \", texto_sin_tildes)\n",
        "    texto_sin_tildes = re.sub(r\"[^a-zA-Z0-9?.!]+\", \" \", texto_sin_tildes)\n",
        "    texto_sin_tildes = texto_sin_tildes.strip()\n",
        "    return texto_sin_tildes\n",
        "\n",
        "# --- 3. Proceso de Lectura y Limpieza ---\n",
        "print(f\"Iniciando el procesamiento de '{FILE_IN}'...\")\n",
        "\n",
        "try:\n",
        "    # --- Cargar el archivo .tsv ---\n",
        "    df = pd.read_csv(\n",
        "        FILE_IN,\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['espanol', 'ingles'],\n",
        "        usecols=[0, 1],\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "\n",
        "    print(f\"Se encontraron {len(df)} pares de frases en total.\")\n",
        "    df = df.dropna()\n",
        "\n",
        "    # --- 4. Muestreo y Limpieza ---\n",
        "    if len(df) > N_SAMPLES:\n",
        "        print(f\"Tomando una muestra aleatoria de {N_SAMPLES} pares...\")\n",
        "        df_sample = df.sample(n=N_SAMPLES, random_state=42)\n",
        "    else:\n",
        "        print(f\"Se usar√°n todos los {len(df)} pares disponibles.\")\n",
        "        df_sample = df\n",
        "\n",
        "    print(\"Limpiando textos (quitando tildes)...\")\n",
        "    df_sample['espanol'] = df_sample['espanol'].apply(limpiar_texto_sin_tildes)\n",
        "    df_sample['ingles'] = df_sample['ingles'].apply(limpiar_texto_sin_tildes)\n",
        "\n",
        "    df_sample = df_sample.dropna()\n",
        "    df_sample = df_sample[\n",
        "        (df_sample['espanol'].str.len() > 0) &\n",
        "        (df_sample['ingles'].str.len() > 0)\n",
        "    ]\n",
        "\n",
        "    # --- 5. Guardar CSV Local ---\n",
        "    df_sample.to_csv(FILE_OUT, index=False)\n",
        "\n",
        "    print(f\"\\n¬°√âxito! Dataset limpio guardado localmente en: {FILE_OUT}\")\n",
        "    print(f\"Total de pares procesados: {len(df_sample)}\")\n",
        "    print(\"\\nVista previa de los datos:\")\n",
        "    print(df_sample.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"¬°ERROR DE ARCHIVO NO ENCONTRADO!\")\n",
        "    print(f\"Aseg√∫rate de haber subido el archivo '{FILE_IN}' a Colab.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurri√≥ un error inesperado: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpL86toCIYNK",
        "outputId": "10c948f8-6399-4a88-85eb-d78da1987949"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando el procesamiento de 'Parejas de oraciones en Espa√±olIngl√©s - 2025-11-18.tsv'...\n",
            "Se encontraron 275858 pares de frases en total.\n",
            "Se usar√°n todos los 275858 pares disponibles.\n",
            "Limpiando textos (quitando tildes)...\n",
            "\n",
            "¬°√âxito! Dataset limpio guardado localmente en: tatoeba_limpio_1M.csv\n",
            "Total de pares procesados: 275858\n",
            "\n",
            "Vista previa de los datos:\n",
            "  espanol                     ingles\n",
            "0    2481          intentemos algo !\n",
            "1    2482  tengo que irme a dormir .\n",
            "2    2483       que estas haciendo ?\n",
            "3    2483       que estas haciendo ?\n",
            "4    2483       que estas haciendo ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import csv\n",
        "from google.colab import drive\n",
        "import unicodedata\n",
        "\n",
        "# --- 0. Montar Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 1. Configuraci√≥n de Rutas ---\n",
        "# Lee el archivo de 1M que cre√≥ la Celda 1\n",
        "DATASET_FILE = \"tatoeba_limpio_1M.csv\"\n",
        "\n",
        "# RUTA DE SALIDA (Guardado)\n",
        "RUTA_BASE_DRIVE = \"/content/drive/My Drive/TraductorRNN_Tatoeba_1M/\"\n",
        "\n",
        "if not os.path.exists(RUTA_BASE_DRIVE):\n",
        "    os.makedirs(RUTA_BASE_DRIVE)\n",
        "    print(f\"Carpeta creada en: {RUTA_BASE_DRIVE}\")\n",
        "\n",
        "# Archivos que se guardar√°n/cargar√°n de Drive\n",
        "MODEL_FILE = os.path.join(RUTA_BASE_DRIVE, 'traductor_rnn.keras')\n",
        "TOKENIZER_SPA_FILE = os.path.join(RUTA_BASE_DRIVE, 'tokenizer_spa.pkl')\n",
        "TOKENIZER_ENG_FILE = os.path.join(RUTA_BASE_DRIVE, 'tokenizer_eng.pkl')\n",
        "MODEL_PARAMS_FILE = os.path.join(RUTA_BASE_DRIVE, 'model_params.pkl')\n",
        "FEEDBACK_FILE = os.path.join(RUTA_BASE_DRIVE, 'feedback.csv')\n",
        "\n",
        "# --- ¬°¬°CAMBIOS PARA AHORRAR MEMORIA!! ---\n",
        "EMBEDDING_DIM = 256\n",
        "LATENT_DIM = 256\n",
        "# 1. Reducimos el tama√±o del lote\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "# 2. A√±adimos una longitud M√ÅXIMA para truncar frases largas\n",
        "MAX_SEQUENCE_LEN = 50\n",
        "# --- FIN DE CAMBIOS DE MEMORIA ---\n",
        "\n",
        "# --- 2. Funciones de Preprocesamiento ---\n",
        "def limpiar_texto_sin_tildes(texto):\n",
        "    \"\"\"Funci√≥n de limpieza para la ENTRADA DEL USUARIO y FEEDBACK.\"\"\"\n",
        "    texto = str(texto).lower()\n",
        "    texto_normalizado = unicodedata.normalize('NFD', texto)\n",
        "    texto_sin_tildes = texto_normalizado.encode('ascii', 'ignore').decode('utf-8')\n",
        "    texto_sin_tildes = re.sub(r\"([?.!,¬ø])\", r\" \\1 \", texto_sin_tildes)\n",
        "    texto_sin_tildes = re.sub(r\"[^a-zA-Z0-9?.!]+\", \" \", texto_sin_tildes)\n",
        "    texto_sin_tildes = texto_sin_tildes.strip()\n",
        "    return texto_sin_tildes\n",
        "\n",
        "def preparar_datos(dataset_path):\n",
        "    \"\"\"Carga el dataset LOCAL y combina el feedback de DRIVE.\"\"\"\n",
        "    print(f\"Cargando y preparando datos desde {dataset_path}...\")\n",
        "    try:\n",
        "        df_main = pd.read_csv(dataset_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"¬°ERROR! No se encontr√≥ el dataset en: {dataset_path}\")\n",
        "        print(\"Por favor, aseg√∫rate de ejecutar la 'Celda 1: Procesar Archivos' primero.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Combinar con feedback\n",
        "    if os.path.exists(FEEDBACK_FILE) and os.path.getsize(FEEDBACK_FILE) > 50:\n",
        "        print(f\"¬°Feedback encontrado en Drive! Combinando...\")\n",
        "        try:\n",
        "            df_feedback = pd.read_csv(FEEDBACK_FILE)\n",
        "            df_feedback['espanol'] = df_feedback['espanol'].apply(limpiar_texto_sin_tildes)\n",
        "            df_feedback['ingles'] = df_feedback['ingles'].apply(limpiar_texto_sin_tildes)\n",
        "            df = pd.concat([df_main, df_feedback], ignore_index=True)\n",
        "            df = df.drop_duplicates(subset=['espanol'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error al combinar feedback: {e}\")\n",
        "            df = df_main\n",
        "    else:\n",
        "        print(\"No se encontr√≥ feedback. Usando solo el dataset original.\")\n",
        "        df = df_main\n",
        "\n",
        "    # Continuar con el preprocesamiento...\n",
        "    espanol_textos = [str(txt) for txt in df['espanol']]\n",
        "    ingles_textos = [f\"[start] {str(txt)} [end]\" for txt in df['ingles']]\n",
        "\n",
        "    tokenizer_spa = Tokenizer(filters='', oov_token='[UNK]')\n",
        "    tokenizer_spa.fit_on_texts(espanol_textos)\n",
        "\n",
        "    tokenizer_eng = Tokenizer(filters='', oov_token='[UNK]')\n",
        "    tokenizer_eng.fit_on_texts(ingles_textos)\n",
        "\n",
        "    with open(TOKENIZER_SPA_FILE, 'wb') as f:\n",
        "        pickle.dump(tokenizer_spa, f)\n",
        "    with open(TOKENIZER_ENG_FILE, 'wb') as f:\n",
        "        pickle.dump(tokenizer_eng, f)\n",
        "\n",
        "    print(f\"Tokenizers guardados en {RUTA_BASE_DRIVE}\")\n",
        "    return espanol_textos, ingles_textos, tokenizer_spa, tokenizer_eng\n",
        "\n",
        "# --- 3. Funci√≥n de Entrenamiento ---\n",
        "def entrenar_y_guardar_modelo(espanol_textos, ingles_textos, tokenizer_spa, tokenizer_eng):\n",
        "    print(\"Iniciando construcci√≥n y entrenamiento del modelo...\")\n",
        "\n",
        "    encoder_input_seq = tokenizer_spa.texts_to_sequences(espanol_textos)\n",
        "    decoder_input_seq = tokenizer_eng.texts_to_sequences(ingles_textos)\n",
        "\n",
        "    decoder_target_seq = [seq[1:] for seq in decoder_input_seq]\n",
        "\n",
        "    # --- ¬°CAMBIO! Usamos la longitud fija ---\n",
        "    max_len_spa = MAX_SEQUENCE_LEN\n",
        "    max_len_eng = MAX_SEQUENCE_LEN\n",
        "\n",
        "    with open(MODEL_PARAMS_FILE, 'wb') as f:\n",
        "        pickle.dump({'max_len_spa': max_len_spa, 'max_len_eng': max_len_eng}, f)\n",
        "\n",
        "    # --- ¬°CAMBIO! A√±adimos 'truncating' para cortar frases largas ---\n",
        "    encoder_input_data = pad_sequences(encoder_input_seq, maxlen=max_len_spa, padding='post', truncating='post')\n",
        "    decoder_input_data = pad_sequences(decoder_input_seq, maxlen=max_len_eng, padding='post', truncating='post')\n",
        "    decoder_target_data = pad_sequences(decoder_target_seq, maxlen=max_len_eng, padding='post', truncating='post')\n",
        "\n",
        "    # Corregimos el 'shape' para sparse_categorical\n",
        "    decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "    num_decoder_tokens = len(tokenizer_eng.word_index) + 1\n",
        "    # --- FIN DE CAMBIOS ---\n",
        "\n",
        "    print(f\"Vocabulario Espa√±ol: {len(tokenizer_spa.word_index) + 1} tokens\")\n",
        "    print(f\"Vocabulario Ingl√©s: {num_decoder_tokens} tokens\")\n",
        "    print(f\"Max. secuencia (FORZADA): {MAX_SEQUENCE_LEN}\")\n",
        "\n",
        "    num_encoder_tokens = len(tokenizer_spa.word_index) + 1\n",
        "\n",
        "    # --- Arquitectura del Modelo ---\n",
        "    encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
        "    enc_embedding_layer = Embedding(num_encoder_tokens, EMBEDDING_DIM, name='encoder_embedding')\n",
        "    enc_embedding_tensor = enc_embedding_layer(encoder_inputs)\n",
        "    encoder_lstm_layer = LSTM(LATENT_DIM, return_state=True, name='encoder_lstm')\n",
        "    _, state_h, state_c = encoder_lstm_layer(enc_embedding_tensor)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
        "    dec_embedding_layer = Embedding(num_decoder_tokens, EMBEDDING_DIM, name='decoder_embedding')\n",
        "    dec_embedding_tensor = dec_embedding_layer(decoder_inputs)\n",
        "    decoder_lstm_layer = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm_layer(dec_embedding_tensor, initial_state=encoder_states)\n",
        "    decoder_dense_layer = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "    decoder_outputs = decoder_dense_layer(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(\"\\nEntrenando... (esto tardar√° varias horas con 1M de filas)\")\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=BATCH_SIZE, # Batch size reducido\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    model.save(MODEL_FILE)\n",
        "    print(f\"Modelo guardado en {MODEL_FILE}\")\n",
        "    return model\n",
        "\n",
        "# --- 4. Funciones de Inferencia ---\n",
        "def crear_modelos_de_inferencia(modelo_entrenado):\n",
        "    print(\"Creando modelos de inferencia...\")\n",
        "\n",
        "    encoder_model_input = Input(shape=(None,), name='encoder_input_inf')\n",
        "    enc_embedding_layer_inf = modelo_entrenado.get_layer('encoder_embedding')\n",
        "    encoder_lstm_layer_inf = modelo_entrenado.get_layer('encoder_lstm')\n",
        "    enc_emb_tensor_inf = enc_embedding_layer_inf(encoder_model_input)\n",
        "    _, state_h_inf, state_c_inf = encoder_lstm_layer_inf(enc_emb_tensor_inf)\n",
        "    encoder_model = Model(encoder_model_input, [state_h_inf, state_c_inf])\n",
        "\n",
        "    decoder_model_input = Input(shape=(1,), name='decoder_input_inf')\n",
        "    decoder_state_h_input = Input(shape=(LATENT_DIM,), name='decoder_state_h_input')\n",
        "    decoder_state_c_input = Input(shape=(LATENT_DIM,), name='decoder_state_c_input')\n",
        "    decoder_states_inputs_inf = [decoder_state_h_input, decoder_state_c_input]\n",
        "\n",
        "    dec_embedding_layer_inf = modelo_entrenado.get_layer('decoder_embedding')\n",
        "    decoder_lstm_layer_inf = modelo_entrenado.get_layer('decoder_lstm')\n",
        "    decoder_dense_layer_inf = modelo_entrenado.get_layer('decoder_dense')\n",
        "\n",
        "    dec_emb_tensor_inf = dec_embedding_layer_inf(decoder_model_input)\n",
        "    dec_outputs_inf, dec_state_h_inf, dec_state_c_inf = decoder_lstm_layer_inf(\n",
        "        dec_emb_tensor_inf, initial_state=decoder_states_inputs_inf\n",
        "    )\n",
        "    dec_final_outputs = decoder_dense_layer_inf(dec_outputs_inf)\n",
        "\n",
        "    decoder_model = Model(\n",
        "        [decoder_input_inf] + decoder_states_inputs_inf,\n",
        "        [dec_final_outputs, dec_state_h_inf, dec_state_c_inf]\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "def traducir_frase(frase_es, encoder_model, decoder_model, tokenizer_spa, tokenizer_eng, max_len_spa, max_len_eng):\n",
        "    frase_limpia = limpiar_texto_sin_tildes(frase_es)\n",
        "    input_seq_raw = tokenizer_spa.texts_to_sequences([frase_limpia])\n",
        "\n",
        "    if not np.any(input_seq_raw[0]) or (len(input_seq_raw[0]) > 0 and all(token == tokenizer_spa.word_index['[UNK]'] for token in input_seq_raw[0])):\n",
        "         return \"[Error: No reconozco ninguna de esas palabras]\"\n",
        "\n",
        "    # Usamos max_len_spa (que ahora es 50) para el padding\n",
        "    input_seq = pad_sequences(input_seq_raw, maxlen=max_len_spa, padding='post', truncating='post')\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer_eng.word_index['[start]']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    idx_a_palabra_eng = {v: k for k, v in tokenizer_eng.word_index.items()}\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = idx_a_palabra_eng.get(sampled_token_index, '[UNK]')\n",
        "\n",
        "        # Usamos max_len_eng (que ahora es 50) para el l√≠mite de traducci√≥n\n",
        "        if (sampled_word == '[end]' or\n",
        "           sampled_word == '[UNK]' or\n",
        "           len(decoded_sentence.split()) > max_len_eng):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += \" \" + sampled_word\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# --- 5. Bucle Principal de Ejecuci√≥n ---\n",
        "def main():\n",
        "    if os.path.exists(MODEL_FILE) and os.path.exists(TOKENIZER_SPA_FILE):\n",
        "        print(\"Cargando modelo y tokenizers existentes desde Drive...\")\n",
        "        modelo_entrenado = load_model(MODEL_FILE)\n",
        "\n",
        "        with open(TOKENIZER_SPA_FILE, 'rb') as f:\n",
        "            tokenizer_spa = pickle.load(f)\n",
        "        with open(TOKENIZER_ENG_FILE, 'rb') as f:\n",
        "            tokenizer_eng = pickle.load(f)\n",
        "        with open(MODEL_PARAMS_FILE, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "            max_len_spa = params['max_len_spa']\n",
        "            max_len_eng = params['max_len_eng']\n",
        "\n",
        "    else:\n",
        "        print(\"No se encontr√≥ un modelo en Drive. Entrenando uno nuevo...\")\n",
        "        datos = preparar_datos(DATASET_FILE)\n",
        "        if datos[0] is None:\n",
        "            return\n",
        "        spa_text, eng_text, tokenizer_spa, tokenizer_eng = datos\n",
        "        modelo_entrenado = entrenar_y_guardar_modelo(\n",
        "            spa_text, eng_text, tokenizer_spa, tokenizer_eng\n",
        "        )\n",
        "        with open(MODEL_PARAMS_FILE, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "            max_len_spa = params['max_len_spa']\n",
        "            max_len_eng = params['max_len_eng']\n",
        "\n",
        "    encoder_model_inf, decoder_model_inf = crear_modelos_de_inferencia(\n",
        "        modelo_entrenado,\n",
        "    )\n",
        "    print(\"¬°Traductor listo!\")\n",
        "\n",
        "    print(\"\\n--- Traductor Interactivo (RNN con 1M de datos) ---\")\n",
        "    print(\"Escribe 'salir' para terminar.\")\n",
        "\n",
        "    while True:\n",
        "        frase_original = input(\"\\nEscribe en espa√±ol (sin tildes): \")\n",
        "        if frase_original.lower() == 'salir':\n",
        "            break\n",
        "        if not frase_original.strip():\n",
        "            continue\n",
        "\n",
        "        traduccion = traducir_frase(\n",
        "            frase_original,\n",
        "            encoder_model_inf,\n",
        "            decoder_model_inf,\n",
        "            tokenizer_spa,\n",
        "            tokenizer_eng,\n",
        "            max_len_spa,\n",
        "            max_len_eng\n",
        "        )\n",
        "        print(f\"Traducci√≥n: {traduccion}\")\n",
        "\n",
        "        feedback = input(\"¬øTraducci√≥n correcta? (s/n): \").lower() # Corregido .cuerpo() a .lower()\n",
        "\n",
        "        if feedback == 'n':\n",
        "            correccion = input(f\"Escribe la traducci√≥n correcta para '{frase_original}': \")\n",
        "            nueva_fila = [frase_original, correccion]\n",
        "            file_exists = os.path.exists(FEEDBACK_FILE)\n",
        "\n",
        "            with open(FEEDBACK_FILE, 'a', encoding='utf-8', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                if not file_exists:\n",
        "                    writer.writerow(['espanol', 'ingles'])\n",
        "                writer.writerow(nueva_fila)\n",
        "            print(f\"¬°Gracias! Correcci√≥n guardada en '{FEEDBACK_FILE}'.\")\n",
        "        elif feedback == 's':\n",
        "            print(\"¬°Genial! üëç\")\n",
        "\n",
        "# --- ¬°Ejecutar todo! ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVQPz1QSLZKQ",
        "outputId": "179ca809-95ad-4ab1-92bd-356f4da2aaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "No se encontr√≥ un modelo en Drive. Entrenando uno nuevo...\n",
            "Cargando y preparando datos desde tatoeba_limpio_1M.csv...\n",
            "No se encontr√≥ feedback. Usando solo el dataset original.\n",
            "Tokenizers guardados en /content/drive/My Drive/TraductorRNN_Tatoeba_1M/\n",
            "Iniciando construcci√≥n y entrenamiento del modelo...\n",
            "Vocabulario Espa√±ol: 254307 tokens\n",
            "Vocabulario Ingl√©s: 48853 tokens\n",
            "Max. secuencia (FORZADA): 50\n",
            "\n",
            "Entrenando... (esto tardar√° varias horas con 1M de filas)\n",
            "Epoch 1/5\n",
            "\u001b[1m5756/6897\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m3:53\u001b[0m 205ms/step - accuracy: 0.8627 - loss: 1.1248"
          ]
        }
      ]
    }
  ]
}