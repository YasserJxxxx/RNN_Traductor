{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdO4rhqIBgsMa5qwfXOYvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasserJxxxx/RNN_Traductor/blob/main/Traductor_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "# --- 1. Vocabulario \"desde cero\" (SIN TILDES) ---\n",
        "\n",
        "# --- FRASES BASICAS ---\n",
        "frases_basicas = [\n",
        "    (\"hola\", \"hello\"),\n",
        "    (\"como estas\", \"how are you\"),\n",
        "    (\"buenos dias\", \"good morning\"),\n",
        "    (\"buenas noches\", \"good night\"),\n",
        "    (\"adios\", \"goodbye\"),\n",
        "    (\"gracias\", \"thank you\"),\n",
        "    (\"por favor\", \"please\"),\n",
        "    (\"si\", \"yes\"),\n",
        "    (\"no\", \"no\"),\n",
        "    (\"como te llamas\", \"what is your name\"),\n",
        "    (\"mi nombre es\", \"my name is\"),\n",
        "    (\"donde estas\", \"where are you\"),\n",
        "    (\"tengo hambre\", \"i am hungry\"),\n",
        "    (\"esta lloviendo\", \"it is raining\")\n",
        "]\n",
        "\n",
        "# --- ESTRUCTURAS GRAMATICALES (Sujeto-Verbo-Objeto) ---\n",
        "sujetos = [\n",
        "    (\"el gato\", \"the cat\"),\n",
        "    (\"el perro\", \"the dog\"),\n",
        "    (\"un nino\", \"a boy\"),\n",
        "    (\"una nina\", \"a girl\"),\n",
        "    (\"el hombre\", \"the man\"),\n",
        "    (\"la mujer\", \"the woman\"),\n",
        "    (\"un pajaro\", \"a bird\"),\n",
        "    (\"el estudiante\", \"the student\")\n",
        "]\n",
        "\n",
        "verbos = [\n",
        "    (\"come\", \"eats\"),\n",
        "    (\"ve\", \"sees\"),\n",
        "    (\"juega con\", \"plays with\"),\n",
        "    (\"quiere\", \"wants\"),\n",
        "    (\"mira\", \"watches\"),\n",
        "    (\"lee\", \"reads\"),\n",
        "    (\"tiene\", \"has\"),\n",
        "    (\"corre hacia\", \"runs to\")\n",
        "]\n",
        "\n",
        "objetos = [\n",
        "    (\"la pelota\", \"the ball\"),\n",
        "    (\"comida\", \"food\"),\n",
        "    (\"un juguete\", \"a toy\"),\n",
        "    (\"un arbol\", \"a tree\"),\n",
        "    (\"el libro\", \"the book\"),\n",
        "    (\"la casa\", \"the house\"),\n",
        "    (\"agua\", \"water\"),\n",
        "    (\"un auto\", \"a car\")\n",
        "]\n",
        "\n",
        "# --- 2. Configuraci贸n ---\n",
        "N_DATOS = 50000\n",
        "ARCHIVO_FINAL_CSV = \"dataset_sintetico_50k.csv\"\n",
        "\n",
        "# Probabilidad de que elijamos una frase basica (ej. 0.3 = 30% de las veces)\n",
        "PROBABILIDAD_FRASE_BASICA = 0.3\n",
        "\n",
        "print(f\"Generando {N_DATOS} pares de oraciones sinteticas (sin tildes)...\")\n",
        "\n",
        "# --- 3. Proceso de generaci贸n y guardado ---\n",
        "\n",
        "filas_generadas = []\n",
        "\n",
        "for _ in range(N_DATOS):\n",
        "\n",
        "    # Decidimos si generar una frase basica o una SVO\n",
        "    if random.random() < PROBABILIDAD_FRASE_BASICA:\n",
        "        # --- Generar Frase Basica ---\n",
        "        par_frase = random.choice(frases_basicas)\n",
        "        frase_es = par_frase[0]\n",
        "        frase_en = par_frase[1]\n",
        "\n",
        "    else:\n",
        "        # --- Generar Frase SVO ---\n",
        "        s = random.choice(sujetos)\n",
        "        v = random.choice(verbos)\n",
        "        o = random.choice(objetos)\n",
        "\n",
        "        # Construimos la frase en espanol e ingles\n",
        "        frase_es = f\"{s[0]} {v[0]} {o[0]}.\"\n",
        "        frase_en = f\"{s[1]} {v[1]} {o[1]}.\"\n",
        "\n",
        "    filas_generadas.append([frase_es, frase_en])\n",
        "\n",
        "# Mezclamos la lista final para que no esten agrupadas\n",
        "random.shuffle(filas_generadas)\n",
        "\n",
        "# --- Guardamos todo en el CSV ---\n",
        "try:\n",
        "    with open(ARCHIVO_FINAL_CSV, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # Escribimos el encabezado (header)\n",
        "        writer.writerow(['espanol', 'ingles'])\n",
        "\n",
        "        # Escribimos todas las filas generadas\n",
        "        writer.writerows(filas_generadas)\n",
        "\n",
        "    print(f\"\\n隆xito! コ\")\n",
        "    print(f\"Dataset sint茅tico guardado en: {ARCHIVO_FINAL_CSV}\")\n",
        "\n",
        "    # Imprimimos 5 ejemplos para verificar\n",
        "    print(\"\\n--- Vista previa de los datos generados ---\")\n",
        "    for i in range(5):\n",
        "        print(filas_generadas[i])\n",
        "\n",
        "except IOError as e:\n",
        "    print(f\"Error al escribir el archivo: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrio un error inesperado: {e}\")"
      ],
      "metadata": {
        "id": "yKp7dLDB9e7X",
        "outputId": "5b5e2e1f-4e4e-4003-d7bb-b16eeeb096db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando 50000 pares de oraciones sinteticas (sin tildes)...\n",
            "\n",
            "隆xito! コ\n",
            "Dataset sint茅tico guardado en: dataset_sintetico_50k.csv\n",
            "\n",
            "--- Vista previa de los datos generados ---\n",
            "['el estudiante lee comida.', 'the student reads food.']\n",
            "['el perro tiene la pelota.', 'the dog has the ball.']\n",
            "['la mujer come la casa.', 'the woman eats the house.']\n",
            "['buenos dias', 'good morning']\n",
            "['un pajaro quiere el libro.', 'a bird wants the book.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import csv\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 0. Montar Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 1. Configuraci贸n de Rutas ---\n",
        "\n",
        "# RUTA DE ENTRADA (Dataset)\n",
        "# Apunta al archivo LOCAL que creaste en la Celda 1\n",
        "DATASET_FILE = \"dataset_sintetico_50k.csv\"\n",
        "\n",
        "# RUTA DE SALIDA (Guardado)\n",
        "# Apunta a tu carpeta de Google Drive\n",
        "RUTA_BASE_DRIVE = \"/content/drive/My Drive/TraductorRNN/\"\n",
        "\n",
        "# Crear la carpeta de Drive si no existe\n",
        "if not os.path.exists(RUTA_BASE_DRIVE):\n",
        "    os.makedirs(RUTA_BASE_DRIVE)\n",
        "    print(f\"Carpeta creada en: {RUTA_BASE_DRIVE}\")\n",
        "\n",
        "# Archivos que se guardar谩n/cargar谩n de Drive\n",
        "MODEL_FILE = os.path.join(RUTA_BASE_DRIVE, 'traductor_rnn.keras')\n",
        "TOKENIZER_SPA_FILE = os.path.join(RUTA_BASE_DRIVE, 'tokenizer_spa.pkl')\n",
        "TOKENIZER_ENG_FILE = os.path.join(RUTA_BASE_DRIVE, 'tokenizer_eng.pkl')\n",
        "MODEL_PARAMS_FILE = os.path.join(RUTA_BASE_DRIVE, 'model_params.pkl')\n",
        "FEEDBACK_FILE = os.path.join(RUTA_BASE_DRIVE, 'feedback.csv')\n",
        "\n",
        "# Par谩metros del modelo\n",
        "EMBEDDING_DIM = 256\n",
        "LATENT_DIM = 256\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "# --- 2. Funciones de Preprocesamiento ---\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r\"([?.!,驴])\", r\" \\1 \", texto)\n",
        "    texto = re.sub(r'[\" \"]+', \" \", texto)\n",
        "    texto = texto.strip()\n",
        "    return texto\n",
        "\n",
        "def preparar_datos(dataset_path):\n",
        "    \"\"\"Carga y procesa el dataset desde la RUTA LOCAL.\"\"\"\n",
        "    print(f\"Cargando y preparando datos desde {dataset_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(dataset_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"隆ERROR! No se encontr贸 el dataset en: {dataset_path}\")\n",
        "        print(\"Por favor, aseg煤rate de ejecutar la 'Celda 1: Generar Dataset' primero.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    espanol_textos = [limpiar_texto(str(txt)) for txt in df['espanol']]\n",
        "    ingles_textos = [f\"[start] {limpiar_texto(str(txt))} [end]\" for txt in df['ingles']]\n",
        "\n",
        "    tokenizer_spa = Tokenizer(filters='')\n",
        "    tokenizer_spa.fit_on_texts(espanol_textos)\n",
        "\n",
        "    tokenizer_eng = Tokenizer(filters='')\n",
        "    tokenizer_eng.fit_on_texts(ingles_textos)\n",
        "\n",
        "    # Guarda los tokenizers en DRIVE\n",
        "    with open(TOKENIZER_SPA_FILE, 'wb') as f:\n",
        "        pickle.dump(tokenizer_spa, f)\n",
        "    with open(TOKENIZER_ENG_FILE, 'wb') as f:\n",
        "        pickle.dump(tokenizer_eng, f)\n",
        "\n",
        "    print(f\"Tokenizers guardados en {RUTA_BASE_DRIVE}\")\n",
        "    return espanol_textos, ingles_textos, tokenizer_spa, tokenizer_eng\n",
        "\n",
        "# --- 3. Funci贸n de Entrenamiento ---\n",
        "\n",
        "def entrenar_y_guardar_modelo(espanol_textos, ingles_textos, tokenizer_spa, tokenizer_eng):\n",
        "    \"\"\"Construye, entrena y guarda el modelo en DRIVE.\"\"\"\n",
        "\n",
        "    print(\"Iniciando construcci贸n y entrenamiento del modelo...\")\n",
        "\n",
        "    encoder_input_seq = tokenizer_spa.texts_to_sequences(espanol_textos)\n",
        "    decoder_input_seq = tokenizer_eng.texts_to_sequences(ingles_textos)\n",
        "\n",
        "    decoder_target_seq = [seq[1:] for seq in decoder_input_seq]\n",
        "\n",
        "    max_len_spa = max(len(s) for s in encoder_input_seq)\n",
        "    max_len_eng = max(len(s) for s in decoder_input_seq)\n",
        "\n",
        "    # Guarda los par谩metros del modelo en DRIVE\n",
        "    with open(MODEL_PARAMS_FILE, 'wb') as f:\n",
        "        pickle.dump({'max_len_spa': max_len_spa, 'max_len_eng': max_len_eng}, f)\n",
        "\n",
        "    encoder_input_data = pad_sequences(encoder_input_seq, maxlen=max_len_spa, padding='post')\n",
        "    decoder_input_data = pad_sequences(decoder_input_seq, maxlen=max_len_eng, padding='post')\n",
        "\n",
        "    decoder_target_data_seq = pad_sequences(decoder_target_seq, maxlen=max_len_eng, padding='post')\n",
        "\n",
        "    num_decoder_tokens = len(tokenizer_eng.word_index) + 1\n",
        "    decoder_target_data = tf.keras.utils.to_categorical(\n",
        "        decoder_target_data_seq, num_classes=num_decoder_tokens\n",
        "    )\n",
        "\n",
        "    print(f\"Vocabulario Espa帽ol: {len(tokenizer_spa.word_index) + 1} tokens\")\n",
        "    print(f\"Vocabulario Ingl茅s: {num_decoder_tokens} tokens\")\n",
        "    print(f\"Max. secuencia Espa帽ol: {max_len_spa}\")\n",
        "    print(f\"Max. secuencia Ingl茅s: {max_len_eng}\")\n",
        "\n",
        "    num_encoder_tokens = len(tokenizer_spa.word_index) + 1\n",
        "\n",
        "    # --- ENCODER ---\n",
        "    encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
        "    enc_embedding_layer = Embedding(num_encoder_tokens, EMBEDDING_DIM, name='encoder_embedding')\n",
        "    enc_embedding_tensor = enc_embedding_layer(encoder_inputs)\n",
        "    encoder_lstm_layer = LSTM(LATENT_DIM, return_state=True, name='encoder_lstm')\n",
        "    _, state_h, state_c = encoder_lstm_layer(enc_embedding_tensor)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # --- DECODER ---\n",
        "    decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
        "    dec_embedding_layer = Embedding(num_decoder_tokens, EMBEDDING_DIM, name='decoder_embedding')\n",
        "    dec_embedding_tensor = dec_embedding_layer(decoder_inputs)\n",
        "    decoder_lstm_layer = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm_layer(dec_embedding_tensor, initial_state=encoder_states)\n",
        "    decoder_dense_layer = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "    decoder_outputs = decoder_dense_layer(decoder_outputs)\n",
        "\n",
        "    # --- Modelo de Entrenamiento ---\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(\"\\nEntrenando...\")\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Guarda el modelo final en DRIVE\n",
        "    model.save(MODEL_FILE)\n",
        "    print(f\"Modelo guardado en {MODEL_FILE}\")\n",
        "    return model\n",
        "\n",
        "# --- 4. Funciones de Inferencia (Traducci贸n) ---\n",
        "\n",
        "def crear_modelos_de_inferencia(modelo_entrenado):\n",
        "    \"\"\"Crea los modelos separados de Encoder y Decoder.\"\"\"\n",
        "\n",
        "    print(\"Creando modelos de inferencia...\")\n",
        "\n",
        "    # --- ENCODER de Inferencia ---\n",
        "    encoder_model_input = Input(shape=(None,), name='encoder_input_inf')\n",
        "    enc_embedding_layer_inf = modelo_entrenado.get_layer('encoder_embedding')\n",
        "    encoder_lstm_layer_inf = modelo_entrenado.get_layer('encoder_lstm')\n",
        "    enc_emb_tensor_inf = enc_embedding_layer_inf(encoder_model_input)\n",
        "    _, state_h_inf, state_c_inf = encoder_lstm_layer_inf(enc_emb_tensor_inf)\n",
        "    encoder_model = Model(encoder_model_input, [state_h_inf, state_c_inf])\n",
        "\n",
        "    # --- DECODER de Inferencia ---\n",
        "    decoder_model_input = Input(shape=(1,), name='decoder_input_inf')\n",
        "    decoder_state_h_input = Input(shape=(LATENT_DIM,), name='decoder_state_h_input')\n",
        "    decoder_state_c_input = Input(shape=(LATENT_DIM,), name='decoder_state_c_input')\n",
        "    decoder_states_inputs_inf = [decoder_state_h_input, decoder_state_c_input]\n",
        "\n",
        "    dec_embedding_layer_inf = modelo_entrenado.get_layer('decoder_embedding')\n",
        "    decoder_lstm_layer_inf = modelo_entrenado.get_layer('decoder_lstm')\n",
        "    decoder_dense_layer_inf = modelo_entrenado.get_layer('decoder_dense')\n",
        "\n",
        "    dec_emb_tensor_inf = dec_embedding_layer_inf(decoder_model_input)\n",
        "    dec_outputs_inf, dec_state_h_inf, dec_state_c_inf = decoder_lstm_layer_inf(\n",
        "        dec_emb_tensor_inf, initial_state=decoder_states_inputs_inf\n",
        "    )\n",
        "    dec_final_outputs = decoder_dense_layer_inf(dec_outputs_inf)\n",
        "\n",
        "    decoder_model = Model(\n",
        "        [decoder_model_input] + decoder_states_inputs_inf,\n",
        "        [dec_final_outputs, dec_state_h_inf, dec_state_c_inf]\n",
        "    )\n",
        "\n",
        "    print(\"Modelos de Encoder y Decoder para inferencia creados.\")\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "def traducir_frase(frase_es, encoder_model, decoder_model, tokenizer_spa, tokenizer_eng, max_len_spa, max_len_eng):\n",
        "    \"\"\"Toma una frase en espa帽ol y la traduce.\"\"\"\n",
        "\n",
        "    frase_limpia = limpiar_texto(frase_es)\n",
        "    input_seq_raw = tokenizer_spa.texts_to_sequences([frase_limpia])\n",
        "    input_seq = pad_sequences(input_seq_raw, maxlen=max_len_spa, padding='post')\n",
        "\n",
        "    if not np.any(input_seq): # Si todas las palabras son desconocidas\n",
        "        return \"[Error: No reconozco ninguna de esas palabras]\"\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer_eng.word_index['[start]']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    idx_a_palabra_eng = {v: k for k, v in tokenizer_eng.word_index.items()}\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = idx_a_palabra_eng.get(sampled_token_index, '[UNK]')\n",
        "\n",
        "        if (sampled_word == '[end]' or\n",
        "           len(decoded_sentence.split()) > max_len_eng):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += \" \" + sampled_word\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# --- 5. Bucle Principal de Ejecuci贸n ---\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Comprueba si el MODELO existe en DRIVE\n",
        "    if os.path.exists(MODEL_FILE) and os.path.exists(TOKENIZER_SPA_FILE):\n",
        "        print(\"Cargando modelo y tokenizers existentes desde Drive...\")\n",
        "        modelo_entrenado = load_model(MODEL_FILE)\n",
        "\n",
        "        with open(TOKENIZER_SPA_FILE, 'rb') as f:\n",
        "            tokenizer_spa = pickle.load(f)\n",
        "        with open(TOKENIZER_ENG_FILE, 'rb') as f:\n",
        "            tokenizer_eng = pickle.load(f)\n",
        "        with open(MODEL_PARAMS_FILE, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "            max_len_spa = params['max_len_spa']\n",
        "            max_len_eng = params['max_len_eng']\n",
        "\n",
        "    else:\n",
        "        print(\"No se encontr贸 un modelo en Drive. Entrenando uno nuevo...\")\n",
        "        # Prepara los datos desde el CSV LOCAL\n",
        "        datos = preparar_datos(DATASET_FILE)\n",
        "        if datos[0] is None:\n",
        "            return # Detener si el CSV no se encontr贸\n",
        "\n",
        "        spa_text, eng_text, tokenizer_spa, tokenizer_eng = datos\n",
        "\n",
        "        # Entrena y GUARDA EN DRIVE\n",
        "        modelo_entrenado = entrenar_y_guardar_modelo(\n",
        "            spa_text, eng_text, tokenizer_spa, tokenizer_eng\n",
        "        )\n",
        "        # Carga los params que acabamos de guardar\n",
        "        with open(MODEL_PARAMS_FILE, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "            max_len_spa = params['max_len_spa']\n",
        "            max_len_eng = params['max_len_eng']\n",
        "\n",
        "    # Crear modelos de Inferencia\n",
        "    encoder_model_inf, decoder_model_inf = crear_modelos_de_inferencia(\n",
        "        modelo_entrenado,\n",
        "    )\n",
        "    print(\"隆Traductor listo!\")\n",
        "\n",
        "    # Bucle de Interacci贸n\n",
        "    print(\"\\n--- Traductor Interactivo (RNN) ---\")\n",
        "    print(\"Escribe 'salir' para terminar.\")\n",
        "\n",
        "    while True:\n",
        "        frase_original = input(\"\\nEscribe en espa帽ol: \")\n",
        "        if frase_original.lower() == 'salir':\n",
        "            break\n",
        "        if not frase_original.strip():\n",
        "            continue\n",
        "\n",
        "        traduccion = traducir_frase(\n",
        "            frase_original,\n",
        "            encoder_model_inf,\n",
        "            decoder_model_inf,\n",
        "            tokenizer_spa,\n",
        "            tokenizer_eng,\n",
        "            max_len_spa,\n",
        "            max_len_eng\n",
        "        )\n",
        "        print(f\"Traducci贸n: {traduccion}\")\n",
        "\n",
        "        # Recolecci贸n de Feedback (se guarda en DRIVE)\n",
        "        feedback = input(\"驴Traducci贸n correcta? (s/n): \").lower()\n",
        "\n",
        "        if feedback == 'n':\n",
        "            correccion = input(f\"Escribe la traducci贸n correcta para '{frase_original}': \")\n",
        "            nueva_fila = [frase_original, correccion]\n",
        "            file_exists = os.path.exists(FEEDBACK_FILE)\n",
        "\n",
        "            with open(FEEDBACK_FILE, 'a', encoding='utf-8', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                if not file_exists:\n",
        "                    writer.writerow(['espanol', 'ingles'])\n",
        "                writer.writerow(nueva_fila)\n",
        "            print(f\"隆Gracias! Correcci贸n guardada en '{FEEDBACK_FILE}'.\")\n",
        "        elif feedback == 's':\n",
        "            print(\"隆Genial! \")\n",
        "\n",
        "# --- 隆Ejecutar todo! ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP9RJajgCE1E",
        "outputId": "d71272cf-4334-44b6-ae97-38b33ae88f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cargando modelo y tokenizers existentes desde Drive...\n",
            "Creando modelos de inferencia...\n",
            "Modelos de Encoder y Decoder para inferencia creados.\n",
            "隆Traductor listo!\n",
            "\n",
            "--- Traductor Interactivo (RNN) ---\n",
            "Escribe 'salir' para terminar.\n",
            "\n",
            "Escribe en espa帽ol: espa帽ol\n",
            "Traducci贸n: [Error: No reconozco ninguna de esas palabras]\n",
            "驴Traducci贸n correcta? (s/n): n\n",
            "Escribe la traducci贸n correcta para 'espa帽ol': ingles\n",
            "隆Gracias! Correcci贸n guardada en '/content/drive/My Drive/TraductorRNN/feedback.csv'.\n",
            "\n",
            "Escribe en espa帽ol: hola\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 25 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d76ac8b4b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traducci贸n: hello\n",
            "驴Traducci贸n correcta? (s/n): s\n",
            "隆Genial! \n",
            "\n",
            "Escribe en espa帽ol: como estas \n",
            "Traducci贸n: how are you\n",
            "驴Traducci贸n correcta? (s/n): s\n",
            "隆Genial! \n",
            "\n",
            "Escribe en espa帽ol: n\n",
            "Traducci贸n: [Error: No reconozco ninguna de esas palabras]\n",
            "驴Traducci贸n correcta? (s/n): n\n",
            "Escribe la traducci贸n correcta para 'n': n\n",
            "隆Gracias! Correcci贸n guardada en '/content/drive/My Drive/TraductorRNN/feedback.csv'.\n",
            "\n",
            "Escribe en espa帽ol: no\n",
            "Traducci贸n: no\n",
            "驴Traducci贸n correcta? (s/n): s\n",
            "隆Genial! \n",
            "\n",
            "Escribe en espa帽ol: listo\n",
            "Traducci贸n: [Error: No reconozco ninguna de esas palabras]\n",
            "驴Traducci贸n correcta? (s/n): s\n",
            "隆Genial! \n",
            "\n",
            "Escribe en espa帽ol: ready\n",
            "Traducci贸n: [Error: No reconozco ninguna de esas palabras]\n",
            "驴Traducci贸n correcta? (s/n): mi nombre es\n",
            "\n",
            "Escribe en espa帽ol: mi nombre es\n",
            "Traducci贸n: my name is\n",
            "驴Traducci贸n correcta? (s/n): s\n",
            "隆Genial! \n",
            "\n",
            "Escribe en espa帽ol: tengo hambre\n",
            "Traducci贸n: i am hungry\n"
          ]
        }
      ]
    }
  ]
}